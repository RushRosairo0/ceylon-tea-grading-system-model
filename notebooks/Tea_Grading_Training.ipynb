{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc19758f",
   "metadata": {},
   "source": [
    "## Notebook: Tea_Grading_Training.ipynb\n",
    "## Folder: notebooks/\n",
    "## Purpose: Interactive training and evaluation for Tea Grading AI Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd74ff1",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17d81b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c06f8",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "099a09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # map tea grades for classification\n",
    "        self.grade_map = {\n",
    "            \"OP\": 0, \n",
    "            \"OP1\": 1, \n",
    "            \"OPA\": 2,\n",
    "            \"NOT_TEA\": 3\n",
    "        }\n",
    "\n",
    "        # go through all main grade folders in the dataset  \n",
    "        for grade_name in os.listdir(root_dir):\n",
    "            grade_path = os.path.join(root_dir, grade_name)\n",
    "\n",
    "            # ignore unknown folder names\n",
    "            if not os.path.isdir(grade_path) or grade_name not in self.grade_map:\n",
    "                continue\n",
    "\n",
    "            grade_label = self.grade_map[grade_name]\n",
    "\n",
    "            # go through subfolders (quality) in the main folder\n",
    "            for quality_folder in os.listdir(grade_path):\n",
    "                if not quality_folder.startswith(\"quality_\"):\n",
    "                    continue\n",
    "\n",
    "                # ignore unknown folder names\n",
    "                quality_path = os.path.join(grade_path, quality_folder)\n",
    "\n",
    "                # convert subfolder names to indexes\n",
    "                quality_num = int(quality_folder.split('_')[1]) - 1\n",
    "\n",
    "                # go through image files in the subfolder\n",
    "                for img_name in os.listdir(quality_path):\n",
    "                    if img_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                        img_path = os.path.join(quality_path, img_name)\n",
    "\n",
    "                        # add image_path, grade_label and quality_label to samples\n",
    "                        self.samples.append((img_path, grade_label, quality_num))\n",
    "\n",
    "        print(f\"total samples found: {len(self.samples)}\")\n",
    "\n",
    "    # get total number of images in the dataset    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # ready images for modal input \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, grade_label, quality_label = self.samples[idx]\n",
    "\n",
    "        # load and convert image into RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, grade_label, quality_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072ef69",
   "metadata": {},
   "source": [
    "### transforms and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d57d9e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples found: 6915\n"
     ]
    }
   ],
   "source": [
    "# define image transformation for model inputs\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# dataset path\n",
    "DATASET_PATH = \"../dataset/images\"\n",
    "\n",
    "# create dataset to train\n",
    "dataset = TeaDataset(DATASET_PATH, transform=transform)\n",
    "\n",
    "# split dataset into train and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# collect grade labels to handle imbalances\n",
    "train_labels = []\n",
    "for idx in train_dataset.indices:\n",
    "    _, grade_label, _ = dataset[idx]\n",
    "    train_labels.append(grade_label)\n",
    "\n",
    "# convert labels to a tensor\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "# boost weight on tea classes\n",
    "class_sample_weights = torch.tensor([3.0, 3.0, 3.0, 1.0])  # OP, OP1, OPA, NOT_TEA\n",
    "\n",
    "# create sampler to balance classes during training\n",
    "sample_weights = class_sample_weights[train_labels]\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# use sampler in DataLoader instead of shuffle\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ba2f0",
   "metadata": {},
   "source": [
    "### model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da8ad179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network for tea grading and quality prediction \n",
    "class TeaNet(nn.Module):\n",
    "    def __init__(self, num_grades=4, num_qualities=10):\n",
    "        super(TeaNet, self).__init__()\n",
    "\n",
    "        # use a pretrained ResNet-18 for feature extraction (to leverage learned visual features)\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "\n",
    "        # get number of features\n",
    "        num_features = self.backbone.fc.in_features\n",
    "\n",
    "        # remove the original ResNet connected layer\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # classification head for grade and quality\n",
    "        self.grade_head = nn.Linear(num_features, num_grades)   # OP, OP1, OPA, NOT_TEA\n",
    "        self.quality_head = nn.Linear(num_features, num_qualities)  # quality 1-10\n",
    "\n",
    "    # get predicted probabilities for tea grade and quality classification\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        grade_out = self.grade_head(features)\n",
    "        quality_out = self.quality_head(features)\n",
    "        \n",
    "        return grade_out, quality_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9cf19",
   "metadata": {},
   "source": [
    "### training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2acf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect device and use GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialize TeaNet model\n",
    "model = TeaNet(num_grades=4, num_qualities=10).to(device)\n",
    "\n",
    "# define weighted loss for tea grade classification\n",
    "class_weights = torch.tensor([1.0, 1.0, 1.0, 0.25]).to(device)\n",
    "criterion_grade = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# define standard cross entropy loss for tea quality prediction\n",
    "criterion_quality = nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimizer to update model parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# number of passes over the training dataset\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0e3492",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfcc9b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/25 - training loss: 202.9434\n",
      "epoch 2/25 - training loss: 18.4998\n",
      "epoch 3/25 - training loss: 8.7080\n",
      "epoch 4/25 - training loss: 6.6258\n",
      "epoch 5/25 - training loss: 7.2024\n",
      "epoch 6/25 - training loss: 5.5314\n",
      "epoch 7/25 - training loss: 4.3386\n",
      "epoch 8/25 - training loss: 3.1436\n",
      "epoch 9/25 - training loss: 1.7929\n",
      "epoch 10/25 - training loss: 20.0953\n",
      "epoch 11/25 - training loss: 3.4812\n",
      "epoch 12/25 - training loss: 2.9847\n",
      "epoch 13/25 - training loss: 3.2718\n",
      "epoch 14/25 - training loss: 0.8392\n",
      "epoch 15/25 - training loss: 1.4619\n",
      "epoch 16/25 - training loss: 4.1413\n",
      "epoch 17/25 - training loss: 9.6820\n",
      "epoch 18/25 - training loss: 4.2149\n",
      "epoch 19/25 - training loss: 4.4106\n",
      "epoch 20/25 - training loss: 4.4802\n",
      "epoch 21/25 - training loss: 1.4725\n",
      "epoch 22/25 - training loss: 0.8164\n",
      "epoch 23/25 - training loss: 0.6545\n",
      "epoch 24/25 - training loss: 0.7505\n",
      "epoch 25/25 - training loss: 0.3486\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # total losses for current epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    # go through training data\n",
    "    for images, grade_labels, quality_labels in train_loader:\n",
    "        # move data to the device (GPU/CPU)\n",
    "        images, grade_labels, quality_labels = images.to(device), grade_labels.to(device), quality_labels.to(device)\n",
    "\n",
    "        # reset gradients from the previous step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get predictions for grade and quality\n",
    "        grade_preds, quality_preds = model(images)\n",
    "\n",
    "        # calculate loss for grade classification\n",
    "        loss_grade = criterion_grade(grade_preds, grade_labels)\n",
    "\n",
    "        # calculate loss for quality classification (only for OP, OP1, OPA)\n",
    "        not_tea_label = 3\n",
    "        mask = (grade_labels != not_tea_label)\n",
    "        if mask.sum() > 0:\n",
    "            loss_quality = criterion_quality(quality_preds[mask], quality_labels[mask])\n",
    "        else:\n",
    "            loss_quality = torch.tensor(0.0, device=device)\n",
    "\n",
    "        loss = loss_grade + loss_quality\n",
    "\n",
    "        # backpropagation to calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate batch loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"epoch {epoch+1}/{epochs} - training loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1824b",
   "metadata": {},
   "source": [
    "### validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4ffa223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grade accuracy: 99.71 %\n",
      "quality accuracy: 99.92 %\n"
     ]
    }
   ],
   "source": [
    "# accuracy calculation counters\n",
    "correct_grade = 0\n",
    "total_grade = 0\n",
    "correct_quality = 0\n",
    "total_quality = 0\n",
    "\n",
    "# disable gradient computation\n",
    "with torch.no_grad():\n",
    "    # go through validation set\n",
    "    for images, grade_labels, quality_labels in val_loader:\n",
    "        # move data to the device (GPU/CPU)\n",
    "        images = images.to(device)\n",
    "        grade_labels = grade_labels.to(device)\n",
    "        quality_labels = quality_labels.to(device)\n",
    "\n",
    "        # pass through the model to get prediction\n",
    "        grade_preds, quality_preds = model(images)\n",
    "\n",
    "        # get predicted grade and quality\n",
    "        _, grade_predicted = torch.max(grade_preds, 1)\n",
    "        _, quality_predicted = torch.max(quality_preds, 1)\n",
    "\n",
    "        # grade accuracy including NOT_TEA\n",
    "        total_grade += grade_labels.size(0)\n",
    "        correct_grade += (grade_predicted == grade_labels).sum().item()\n",
    "\n",
    "        # quality accuracy for real tea images\n",
    "        mask = (grade_labels != 3)\n",
    "        if mask.sum() > 0:\n",
    "            # count correct quality predictions (only for tea images)\n",
    "            correct_quality += (quality_predicted[mask] == quality_labels[mask]).sum().item()\n",
    "            total_quality += mask.sum().item()\n",
    "\n",
    "print(f\"grade accuracy: {100 * correct_grade / total_grade:.2f} %\")\n",
    "if total_quality > 0:\n",
    "    print(f\"quality accuracy: {100 * correct_quality / total_quality:.2f} %\")\n",
    "else:\n",
    "    print(\"no tea samples found for quality evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2cd08",
   "metadata": {},
   "source": [
    "### save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "882a3c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# create folder to save the model if doesnt exist\n",
    "os.makedirs(\"../saved_models\", exist_ok=True)\n",
    "\n",
    "# save trained model\n",
    "# torch.save(model.state_dict(), \"../saved_models/tea_grading_model.pth\")\n",
    "# torch.save(model.state_dict(), \"../saved_models/tea_grading_model_v2.pth\")\n",
    "torch.save(model.state_dict(), \"../saved_models/tea_grading_model_v3.pth\")\n",
    "\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6a2ac",
   "metadata": {},
   "source": [
    "### test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "445f6588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('OP1', 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict grade and quality for a single image\n",
    "def predict(image_path, model):\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # load image and convert to RGB \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # apply training transformations\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        # pass through the model\n",
    "        grade_out, quality_out = model(image)\n",
    "\n",
    "        # get predicted grade and quality\n",
    "        grade = torch.argmax(grade_out,1).item()\n",
    "        quality = torch.argmax(quality_out,1).item() + 1\n",
    "\n",
    "    # map grade index to text \n",
    "    grade_map = {0:\"OP\", 1:\"OP1\", 2:\"OPA\"}\n",
    "\n",
    "    return grade_map[grade], quality\n",
    "\n",
    "#predict(\"../dataset/images/OP/quality_1/DSC00145.JPG\", model)\n",
    "# predict(\"../dataset/images/OPA/quality_7/DSC03637.JPG\", model)\n",
    "predict(\"../dataset/images/OP1/quality_4/DSC08756.JPG\", model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
